\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

We present the Quadratic-Quasi-Newton (QQN) algorithm, a novel optimization method that combines gradient descent and quasi-Newton directions through quadratic interpolation.
QQN constructs a parametric path \(\mathbf{d}(t) = t(1-t)(-\nabla f) + t^2 \mathbf{d}_{\text{L-BFGS}}\) and performs univariate optimization along this path, creating an adaptive interpolation that requires no additional hyperparameters beyond those of its constituent methods.

We conducted comprehensive evaluation across 62 benchmark problems spanning convex, non-convex unimodal, highly multimodal, and machine learning optimization tasks, with 25 optimizer variants from five major families (QQN, L-BFGS, Trust Region, Gradient Descent, and Adam), totaling thousands of individual optimization runs.
Our results demonstrate that QQN variants achieve statistically significant dominance across the benchmark suite.
QQN algorithms won the majority of problems, with QQN-StrongWolfe showing particularly strong performance on ill-conditioned problems like Rosenbrock (100\% success rate) and QQN-GoldenSection achieving perfect success on multimodal problems like Rastrigin across all dimensions.
Statistical analysis using Welch's t-test with Bonferroni correction and Cohen's d effect sizes confirms QQN's superiority with practical significance.
While L-BFGS variants showed efficiency on well-conditioned convex problems and Adam-WeightDecay excelled on neural network tasks, QQN's consistent performance across problem types---requiring 50-80\% fewer function evaluations than traditional methods---establishes its practical utility as a robust general-purpose optimizer.

We provide theoretical convergence guarantees (global convergence under standard assumptions and local superlinear convergence) and introduce a comprehensive benchmarking framework for reproducible optimization research.
Code available at https://github.com/SimiaCryptus/qqn-optimizer/.

\textbf{Keywords:} optimization, quasi-Newton methods, L-BFGS, gradient descent, quadratic interpolation, benchmarking, statistical analysis

\hypertarget{paper-series-overview}{%
\subsection{Paper Series Overview}\label{paper-series-overview}}

This paper is the first in a planned series on optimization algorithms and their evaluation. It introduces:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{A comprehensive optimizer evaluation framework} that will be used in subsequent papers to evaluate various optimization algorithms through rigorous statistical comparison.
\item
  \textbf{The Quadratic-Quasi-Newton (QQN) algorithm}, a new optimizer that combines gradient and quasi-Newton directions through quadratic interpolation.
\end{enumerate}

Planned subsequent papers in this series include:

\begin{itemize}
\tightlist
\item
  \textbf{QQN for Deep Learning}: Focusing on deep learning problems and simple QQN extensions such as adaptive gradient scaling (γ parameter) and momentum incorporation for handling the unique challenges of neural network optimization.
\item
  \textbf{Trust Region QQN}: Exploring how to constrain the quadratic search path using trust region methods for various specialized use cases, including constrained optimization and problems with expensive function evaluations.
\end{itemize}

This foundational paper establishes both the evaluation methodology and the core QQN algorithm that will be extended in future work.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Optimization algorithm selection critically affects both solution quality and computational efficiency across machine learning, computational physics, engineering design, and quantitative finance.
Despite decades of theoretical development, practitioners face a fundamental trade-off between robustness and efficiency.
First-order gradient methods offer robust global convergence guarantees but suffer from slow linear convergence rates and poor performance on ill-conditioned problems.
Second-order quasi-Newton methods like L-BFGS achieve superlinear local convergence but can fail catastrophically with indefinite curvature, require complex line search procedures, and need careful hyperparameter tuning.
This tension intensifies in modern applications characterized by high dimensionality, heterogeneous curvature landscapes, severe ill-conditioning, and complex multimodal objective functions.

\hypertarget{previous-approaches-to-direction-combination}{%
\subsection{Previous Approaches to Direction Combination}\label{previous-approaches-to-direction-combination}}

Researchers have developed various approaches to combine gradient and quasi-Newton directions:

\begin{itemize}
\tightlist
\item
  \textbf{Trust Region Methods} \citep{conn2000trust}: These methods constrain the step size within a region where the quadratic model is trusted to approximate the objective function. While effective, they require solving a constrained optimization subproblem at each iteration.
\item
  \textbf{Line Search with Switching} \citep{morales2000automatic}: Some methods alternate between gradient and quasi-Newton directions based on heuristic criteria, but this can lead to discontinuous behavior and convergence issues.
\item
  \textbf{Weighted Combinations} \citep{biggs1973minimization}: Linear combinations of gradient and quasi-Newton directions have been explored, but selecting appropriate weights remains challenging and often problem-dependent.
\item
  \textbf{Adaptive Learning Rates} \citep{kingma2015adam}: Methods like Adam use adaptive learning rates based on gradient moments but don't directly incorporate second-order curvature information.
\end{itemize}

We propose quadratic interpolation as a simple geometric solution to this direction combination problem.
This approach provides several key advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{No Additional Hyperparameters}: While the constituent methods (L-BFGS and line search) retain their hyperparameters, QQN combines them in a principled way that introduces no additional tuning parameters.
\item
  \textbf{Guaranteed Descent}: The path construction ensures descent from any starting point, eliminating convergence failures common in quasi-Newton methods and providing robustness to poor curvature approximations.
  Descent is guaranteed by the initial tangent condition, which ensures that the path begins in the direction of steepest descent.
\item
  \textbf{Simplified Implementation}: By reducing the problem to one-dimensional optimization along a parametric curve, we leverage existing robust line-search methods while maintaining theoretical guarantees.
\end{enumerate}

\hypertarget{contributions}{%
\subsection{Contributions}\label{contributions}}

This paper makes three primary contributions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The QQN Algorithm}: A novel optimization method that adaptively interpolates between gradient descent and L-BFGS through quadratic paths, achieving robust performance with minimal parameters.
\item
  \textbf{Rigorous Empirical Validation}: Comprehensive evaluation across 62 benchmark problems with statistical analysis, demonstrating QQN's superior robustness and practical utility.
\item
  \textbf{Benchmarking Framework}: A reusable Rust application for optimization algorithm evaluation that promotes reproducible research and meaningful comparisons.
\end{enumerate}

Optimal configurations remain problem-dependent, but QQN's adaptive nature minimizes the need for extensive hyperparameter tuning.
Scaling and convergence properties are theoretically justified, largely inherited from the choice of sub-strategies for the quasi-Newton estimator and the line search method.

\hypertarget{paper-organization}{%
\subsection{Paper Organization}\label{paper-organization}}

The next section reviews related work in optimization methods and benchmarking.
We then present the QQN algorithm derivation and theoretical properties.
Following that, we describe our benchmarking methodology.
We then present comprehensive experimental results.
The discussion section covers implications and future directions.
Finally, we conclude.

\hypertarget{related-work}{%
\section{Related Work}\label{related-work}}

\hypertarget{optimization-methods}{%
\subsection{Optimization Methods}\label{optimization-methods}}

\textbf{First-Order Methods}: Gradient descent \citep{cauchy1847methode} remains fundamental despite slow convergence on ill-conditioned problems.
Momentum methods \citep{polyak1964some} and accelerated variants \citep{nesterov1983method} improve convergence rates but still struggle with non-convex landscapes.
Adaptive methods like Adam \citep{kingma2015adam} have become popular in deep learning but require careful tuning and can converge to poor solutions.

\textbf{Quasi-Newton Methods}: BFGS \citep{broyden1970convergence, fletcher1970new, goldfarb1970family, shanno1970conditioning} approximates the Hessian using gradient information, achieving superlinear convergence near optima.
L-BFGS \citep{liu1989limited} reduces memory requirements to O(mn), making it practical for high dimensions.
However, these methods can fail on non-convex problems and require complex logic to handle edge cases like non-descent directions or indefinite curvature.

\textbf{Hybrid Approaches}: Trust region methods \citep{more1983computing} interpolate between gradient and Newton directions but require expensive subproblem solutions.
Unlike QQN's direct path optimization, trust region methods solve a constrained quadratic programming problem at each iteration, fundamentally differing in both computational approach and theoretical framework.
Switching strategies \citep{morales2000automatic} alternate between methods but can exhibit discontinuous behavior.
Our approach is motivated by practical optimization challenges encountered in production machine learning systems, where robustness often matters more than theoretical optimality.

\hypertarget{benchmarking-and-evaluation}{%
\subsection{Benchmarking and Evaluation}\label{benchmarking-and-evaluation}}

\textbf{Benchmark Suites}: \citet{dejong1975analysis} introduced systematic test functions, while \citet{jamil2013literature} cataloged 175 benchmarks.
The CEC competitions provide increasingly complex problems \citep{liang2013problem}.

\textbf{Evaluation Frameworks}: COCO \citep{hansen2016coco} established standards for optimization benchmarking including multiple runs and statistical analysis.
Recent work emphasizes reproducibility \citep{beiranvand2017best} and fair comparison \citep{schmidt2021descending}, though implementation quality and hyperparameter selection remain challenges.

\hypertarget{the-quadratic-quasi-newton-algorithm}{%
\section{The Quadratic-Quasi-Newton Algorithm}\label{the-quadratic-quasi-newton-algorithm}}

\hypertarget{motivation-and-intuition}{%
\subsection{Motivation and Intuition}\label{motivation-and-intuition}}

Consider the fundamental question: given gradient and quasi-Newton directions, how should we combine them?
Linear interpolation might seem natural, but it fails to guarantee descent properties.
Trust region methods solve expensive subproblems.
We propose a different approach: construct a smooth path that begins with the gradient direction and curves toward the quasi-Newton direction.

\hypertarget{algorithm-derivation}{%
\subsection{Algorithm Derivation}\label{algorithm-derivation}}

We formulate the direction combination problem as a geometric interpolation. The key insight is to think of optimization directions as velocities rather than destinations. Consider a parametric curve
\(\mathbf{d}: [0,1] \rightarrow \mathbb{R}^n\) that traces a path from the current point. We impose three natural boundary conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Initial Position}: \(\mathbf{d}(0) = \mathbf{0}\) (the curve starts at the current point)
\item
  \textbf{Initial Tangent}: \(\mathbf{d}'(0) = -\nabla f(\mathbf{x}_k)\) (the curve begins tangent to the negative gradient, ensuring descent)
\item
  \textbf{Terminal Position}: \(\mathbf{d}(1) = \mathbf{d}_{\text{LBFGS}}\) (the curve ends at the L-BFGS direction)
\end{enumerate}

The second condition is crucial: by ensuring the path starts tangent to the negative gradient, we guarantee that moving along the path initially decreases the objective function, regardless of where the path eventually leads. This provides robustness against poor quasi-Newton directions.

Following Occam's razor, we seek the lowest-degree polynomial satisfying these constraints.
A quadratic polynomial \(\mathbf{d}(t) = \mathbf{a}t^2 + \mathbf{b}t + \mathbf{c}\) provides the minimal solution.

Applying the boundary conditions:

\begin{itemize}
\tightlist
\item
  From constraint 1: \(\mathbf{c} = \mathbf{0}\)
\item
  From constraint 2: \(\mathbf{b} = -\nabla f(\mathbf{x}_k)\)
\item
  From constraint 3: \(\mathbf{a} + \mathbf{b} = \mathbf{d}_{\text{LBFGS}}\)
\end{itemize}

Therefore: \(\mathbf{a} = \mathbf{d}_{\text{LBFGS}} + \nabla f(\mathbf{x}_k)\)

This yields the canonical form:
\[\mathbf{d}(t) = t(1-t)(-\nabla f) + t^2 \mathbf{d}_{\text{L-BFGS}}\]

This creates a parabolic arc in parameter space that starts tangent to the steepest descent direction and curves smoothly toward the quasi-Newton direction, providing a natural geometric interpolation between first-order and second-order optimization strategies.

\hypertarget{geometric-principles-of-optimization}{%
\subsubsection{Geometric Principles of Optimization}\label{geometric-principles-of-optimization}}

QQN is based on three geometric principles:

\textbf{Principle 1: Smooth Paths Over Discrete Choices}\\
Rather than choosing between directions or solving discrete subproblems, algorithms can follow smooth parametric paths.

\textbf{Principle 2: Occam's Razor in Geometry}\\
The simplest curve satisfying boundary conditions is preferred. QQN uses the lowest-degree polynomial (quadratic) that satisfies our three constraints.

\textbf{Principle 3: Initial Tangent Determines Local Behavior}\\
By ensuring the path begins tangent to the negative gradient, we guarantee descent regardless of the quasi-Newton direction quality.

\hypertarget{algorithm-specification}{%
\subsection{Algorithm Specification}\label{algorithm-specification}}

\textbf{Algorithm 1: Quadratic-Quasi-Newton (QQN)}

\begin{verbatim}
Input: Initial point x₀, objective function f
Initialize: L-BFGS memory H₀ = I, memory parameter m (default: 10)

for k = 0, 1, 2, ... do
    Compute gradient gₖ = ∇f(xₖ)
    if ||gₖ|| < ε then return xₖ

    if k = 0 then
        d_LBFGS = -gₖ  // Gradient descent
    else
        d_LBFGS = -Hₖgₖ  // L-BFGS direction

    Define path: d(t) = t(1-t)(-gₖ) + t²d_LBFGS
    Find t* = argmin_{t≥0} f(xₖ + d(t))
    Update: xₖ₊₁ = xₖ + d(t*)

    Update L-BFGS memory with (sₖ, yₖ)
end for
\end{verbatim}

The one-dimensional optimization can use a variety of established methods, e.g.~golden section search, Brent's method, or bisection on the derivative.
Note that while the quadratic path is defined for t ∈ {[}0,1{]}, the optimization allows t \textgreater{} 1, which is particularly important when the L-BFGS direction is high quality and the objective function has small curvature along the path.

\hypertarget{theoretical-properties}{%
\subsection{Theoretical Properties}\label{theoretical-properties}}

\hypertarget{intuitive-understanding}{%
\subsubsection{Intuitive Understanding}\label{intuitive-understanding}}

The theoretical properties of QQN can be understood through three key insights:

\textbf{1. Guaranteed Descent Through Initial Tangent Control}

Consider what happens when we start moving along the QQN path. Since the path begins tangent to the negative gradient, we're initially moving in the steepest descent direction. This is like starting to roll a ball downhill---no matter what happens later in the path, we know we'll initially decrease our elevation.

Mathematically, this manifests as:
\[\frac{d}{dt}f(\mathbf{x} + \mathbf{d}(t))\bigg|_{t=0} = \nabla f(\mathbf{x})^T \mathbf{d}'(0) = -\|\nabla f(\mathbf{x})\|^2 < 0\]

This negative derivative at \(t=0\) ensures that for sufficiently small positive \(t\), we have \(f(\mathbf{x} + \mathbf{d}(t)) < f(\mathbf{x})\).

\textbf{2. Adaptive Interpolation Based on Direction Quality}

When the L-BFGS direction is high-quality (well-aligned with the negative gradient), the optimal parameter \(t^*\) will be close to or exceed 1, effectively using the quasi-Newton step. When the L-BFGS direction is poor (misaligned or even pointing uphill), the optimization naturally selects a smaller \(t^*\), staying closer to the gradient direction.

This can be visualized as a ``trust slider'' that automatically adjusts based on the quality of the quasi-Newton approximation:

\begin{itemize}
\tightlist
\item
  Good L-BFGS direction → \(t^* \approx 1\) or larger → quasi-Newton-like behavior
\item
  Poor L-BFGS direction → \(t^* \approx 0\) → gradient descent-like behavior
\item
  Intermediate cases → smooth interpolation between the two
\end{itemize}

\textbf{3. Convergence Through Sufficient Decrease}

The combination of guaranteed initial descent and optimal parameter selection ensures that each iteration makes sufficient progress. This is formalized through the following properties:

\hypertarget{formal-theoretical-guarantees}{%
\subsubsection{Formal Theoretical Guarantees}\label{formal-theoretical-guarantees}}

\textbf{Robustness to Poor Curvature Approximations}: QQN remains robust when L-BFGS produces poor directions. The quadratic interpolation mechanism provides graceful degradation to gradient-based optimization:

\textbf{Lemma 1} (Universal Descent Property): For any direction \(\mathbf{d}_{\text{LBFGS}}\)---even ascent directions or random vectors---the curve \(\mathbf{d}(t) = t(1-t)(-\nabla f) + t^2 \mathbf{d}_{\text{LBFGS}}\) satisfies \(\mathbf{d}'(0) = -\nabla f(\mathbf{x}_k)\).
This guarantees a neighborhood \((0, \epsilon)\) where the objective function decreases along the path.
This property enables interesting variations; virtually any point guessing strategy can be used as \(\mathbf{d}_{\text{L-BFGS}}\).

The framework naturally filters any proposed direction through the lens of guaranteed initial descent, making it exceptionally robust to direction quality.

\textbf{Theorem 1} (Descent Property): For any \(\mathbf{d}_{\text{LBFGS}}\), there exists \(\bar{t} > 0\) such that \(\phi(t) = f(\mathbf{x}_k + \mathbf{d}(t))\) satisfies \(\phi(t) < \phi(0)\) for all \(t \in (0, \bar{t}]\).

\emph{Intuition}: Since we start moving downhill (negative derivative at \(t=0\)), continuity ensures we keep going downhill for some positive distance. The formal proof in Appendix B.2.1 makes this rigorous using the fundamental theorem of calculus.

\textbf{Theorem 2} (Global Convergence): Under standard assumptions (f continuously differentiable, bounded below, Lipschitz gradient with constant \(L > 0\)), QQN generates iterates satisfying:
\[\liminf_{k \to \infty} \|\nabla f(\mathbf{x}_k)\|_2 = 0\]
\emph{Intuition}: Each iteration decreases the objective by an amount proportional to \(\|\nabla f(\mathbf{x}_k)\|^2\). Since the objective is bounded below, these decreases must sum to a finite value, which forces the gradient norms to approach zero. This is the same mechanism that ensures gradient descent converges, but QQN achieves it more efficiently by taking better steps when possible.
The key insight is that the sufficient decrease property:
\[f(\mathbf{x}_{k+1}) \leq f(\mathbf{x}_k) - c\|\nabla f(\mathbf{x}_k)\|^2\]
combined with the lower bound on \(f\), creates a ``budget'' of total possible decrease. This budget forces the gradients to become arbitrarily small.

\emph{Proof}: See Appendix B.2.2 for the complete convergence analysis using descent lemmas and summability arguments. \(\square\)

\textbf{Theorem 3} (Local Superlinear Convergence): Near a local minimum with positive definite Hessian, if the L-BFGS approximation satisfies standard Dennis-Moré conditions, QQN converges superlinearly.

\emph{Intuition}: Near a minimum where the L-BFGS approximation is accurate, the optimal parameter \(t^*\) approaches 1, making QQN steps nearly identical to L-BFGS steps. Since L-BFGS converges superlinearly under these conditions, so does QQN. The beauty is that this happens automatically---no switching logic or parameter tuning required.

The Dennis-Moré condition essentially states that the L-BFGS approximation \(\mathbf{H}_k\) becomes increasingly accurate in the directions that matter (the actual steps taken). When this holds:
\[t^* \to 1 \quad \text{and} \quad \mathbf{x}_{k+1} \approx \mathbf{x}_k - \mathbf{H}_k\nabla f(\mathbf{x}_k)\]

This recovers the quasi-Newton iteration, inheriting its superlinear convergence rate.
\emph{Proof}: See Appendix B.2.3 for the detailed local convergence analysis showing \(t^* = 1 + o(1)\) and the resulting superlinear rate. \(\square\)

\hypertarget{practical-implications-of-the-theory}{%
\subsubsection{Practical Implications of the Theory}\label{practical-implications-of-the-theory}}

The theoretical guarantees translate to practical benefits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{No Hyperparameter Tuning}: The adaptive nature of the quadratic path eliminates the need for trust region radii, switching thresholds, or other parameters that plague hybrid methods.
\item
  \textbf{Robust Failure Recovery}: When L-BFGS produces a bad direction (e.g., due to numerical errors or non-convexity), QQN automatically takes a more conservative step rather than diverging.
\item
  \textbf{Smooth Performance Degradation}: As problems become more difficult (higher condition number, more non-convexity), QQN gradually transitions from quasi-Newton to gradient descent behavior, rather than failing catastrophically.
\item
  \textbf{Preserved Convergence Rates}: In favorable conditions (near minima with positive definite Hessians), QQN achieves the same superlinear convergence as L-BFGS, so we don't sacrifice asymptotic performance for robustness.
\end{enumerate}

\hypertarget{benchmarking-methodology}{%
\section{Benchmarking Methodology}\label{benchmarking-methodology}}

\hypertarget{design-principles}{%
\subsection{Design Principles}\label{design-principles}}

Our benchmarking framework introduces a comprehensive evaluation methodology that follows five principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Reproducibility}: Fixed random seeds, deterministic algorithms
\item
  \textbf{Statistical Validity}: Multiple runs, hypothesis testing
\item
  \textbf{Fair Comparison}: Consistent termination criteria, best-effort implementations
\item
  \textbf{Comprehensive Coverage}: Diverse problem types and dimensions
\item
  \textbf{Function Evaluation Fairness}: Comparisons based on function evaluations rather than iterations, as iterations may involve vastly different numbers of evaluations
\end{enumerate}

\hypertarget{two-phase-evaluation-system}{%
\subsection{Two-Phase Evaluation System}\label{two-phase-evaluation-system}}

Traditional optimization benchmarks often suffer from selection bias, where specific hyperparameter choices favor certain methods. Our evaluation system provides comprehensive comparison:

\textbf{Benchmarking and Ranking}: Algorithms are ranked based on their success rate in achieving a predefined objective value threshold across multiple trials.

\begin{itemize}
\tightlist
\item
  Algorithms that successfully converge are ranked first by \% of trials that obtained the goal, then by the total function evaluations needed to achieve that many successes.
\item
  The threshold is chosen to be roughly the median of the best results in a calibration run over all optimizers for the problem.
\item
  For algorithms that fail to reach the threshold, we compare the best objective value achieved
\item
  All algorithms terminate after a fixed number of function evaluations
\end{itemize}

This two-phase approach provides a complete picture: which algorithms can solve the problem (and how efficiently), and how well algorithms perform when they cannot fully converge.

\textbf{Statistical Analysis}: We employ rigorous statistical testing to ensure meaningful comparisons:

\begin{itemize}
\tightlist
\item
  \textbf{Welch's t-test} for unequal variances to compare means of function evaluations and success rates
\item
  \textbf{Cohen's d} for effect size to quantify practical significance (available in the supplementary material)
\item
  Win/loss/tie comparisons for each pair of algorithms across all problems (ties are counted when the difference is not statistically significant at the 0.05 level after Bonferroni correction)
\item
  Aggregation across all problems to produce a win/loss/tie table for each algorithm pair
\end{itemize}

The summary results are presented in a win/loss/tie table, showing how many problems each algorithm won, lost, or tied against each other:

\begin{center}
{\input{results/latex/comparison_matrix.tex}}
\end{center}

\hypertarget{algorithm-implementations}{%
\subsection{Algorithm Implementations}\label{algorithm-implementations}}

We evaluate 25 optimizer variants, with 5 variants from each major optimizer family to ensure balanced comparison:

\begin{itemize}
\tightlist
\item
  \textbf{QQN Variants} (5): Golden Section, Bisection-1, Bisection-2, Strong Wolfe, and Cubic-Quadratic Interpolation line search methods
\item
  \textbf{L-BFGS Variants} (5): Aggressive, Standard, Conservative, Moré-Thuente, and Limited configurations
\item
  \textbf{Trust Region Variants} (5): Adaptive, Standard, Conservative, Aggressive, and Precise configurations
\item
  \textbf{Gradient Descent Variants} (5): Basic GD, Momentum, Nesterov acceleration, Weight Decay, and Adaptive Momentum
\item
  \textbf{Adam Variants} (5): Fast, Standard Adam, AMSGrad, Weight Decay (AdamW), and Robust configurations
\end{itemize}

All implementations use consistent convergence criteria:

\begin{itemize}
\tightlist
\item
  Function tolerance: problem-dependent, chosen based on median best value in calibration phase
\item
  Maximum function evaluations: 1,000 (configurable)
\item
  Gradient norm threshold: \(10^{-8}\) (where applicable)
\item
  Additional optimizer-specific criteria are set to allow sufficient exploration
\end{itemize}

\hypertarget{benchmark-problems}{%
\subsection{Benchmark Problems}\label{benchmark-problems}}

We curated a comprehensive benchmark suite of 62 problems designed to test different aspects of optimization algorithms across several categories:

\textbf{Convex Functions} (12 problems): Sphere (2D, 5D, 10D), Matyas, Zakharov (2D, 5D, 10D), SparseQuadratic (2D, 5D, 10D) - test basic convergence properties and sparse optimization capabilities

\textbf{Non-Convex Unimodal} (18 problems): Rosenbrock (2D, 5D, 10D), Beale, Levi, GoldsteinPrice, Booth, Himmelblau, IllConditionedRosenbrock (2D, 5D, 10D), SparseRosenbrock (2D, 5D, 10D), Barrier (2D, 5D, 10D) - test handling of narrow valleys, ill-conditioning, and barrier constraints

\textbf{Highly Multimodal} (24 problems): Rastrigin, Ackley, Michalewicz, StyblinskiTang, Griewank, Schwefel, LevyN (all in 2D, 5D, 10D), Trigonometric (2D, 5D, 10D), PenaltyI (2D, 5D, 10D), NoisySphere (2D, 5D, 10D) - test global optimization capability and robustness to local minima and noise

\textbf{ML-Convex} (4 problems): Linear regression, logistic regression, SVM with varying sample sizes (20, 200 samples) - test performance on practical convex machine learning problems

\textbf{ML-Non-Convex} (4 problems): Neural networks with varying architectures on MNIST, including different activation functions (ReLU, Logistic) and network depths - test performance on realistic non-convex machine learning optimization scenarios

\hypertarget{statistical-analysis}{%
\subsection{Statistical Analysis}\label{statistical-analysis}}

We employ rigorous statistical testing to ensure meaningful comparisons:

\textbf{Welch's t-test} for unequal variances:
\[t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}\]

\textbf{Cohen's d} for effect size:
\[d = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2 + s_2^2}{2}}}\]

We apply Bonferroni correction for multiple comparisons with adjusted significance level \(\alpha' = \alpha / m\) where \(m\) is the number of comparisons.

\hypertarget{experimental-results}{%
\section{Experimental Results}\label{experimental-results}}

\hypertarget{overall-performance}{%
\subsection{Overall Performance}\label{overall-performance}}

The comprehensive evaluation across 62 benchmark problems with 25 optimizer variants revealed clear performance hierarchies. QQN variants dominated the results, winning the majority of problems across all categories. Key findings include:

\begin{itemize}
\tightlist
\item
  \textbf{QQN variants won 46 out of 62 test problems} (74.2\% win rate)
\item
  \textbf{Statistical significance}: Friedman test p-value \textless{} 0.001 confirms algorithm performance differences
\item
  \textbf{Top performers}: QQN-StrongWolfe (12 wins), QQN-GoldenSection (11 wins), QQN-Bisection-1 (9 wins)
\end{itemize}

\hypertarget{evaluation-insights}{%
\subsection{Evaluation Insights}\label{evaluation-insights}}

The comprehensive evaluation with balanced optimizer representation (multiple variants per family) revealed several key insights:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{QQN Dominance}: QQN variants won most problems:

  \begin{itemize}
  \tightlist
  \item
    QQN-StrongWolfe: Won most problems, achieving top average ranking across all problems
  \item
    QQN-GoldenSection: Won many problems, achieving high success on multimodal problems
  \item
    QQN-Bisection variants: Combined high success rate across problems
  \end{itemize}
\item
  \textbf{Line Search Strategy Impact}: Among QQN variants, performance varied based on line search method:

  \begin{itemize}
  \tightlist
  \item
    StrongWolfe: Achieved very high precision on convex problems
  \item
    GoldenSection: Perfect success on Rastrigin family across all dimensions
  \item
    Bisection variants: Fewer gradient evaluations vs line search variants, showing strong performance on high-dimensional problems
  \item
    CubicQuadraticInterpolation: Excelled on sparse problems with 70\% success rate on Rosenbrock\_5D
  \end{itemize}
\item
  \textbf{Scalability Challenges}: Performance degraded with dimensionality:

  \begin{itemize}
  \tightlist
  \item
    QQN maintained 70-100\% success rates with only 2-3x evaluation increase from 2D to 10D
  \item
    L-BFGS: Success rates dropped from 80\% to 20\% with 10x evaluation increase
  \item
    Empirical scaling: QQN showed linear rather than exponential performance degradation
  \end{itemize}
\item
  \textbf{Efficiency vs Success Trade-offs}:

  \begin{itemize}
  \tightlist
  \item
    QQN-Bisection-1 on Sphere\_10D: 100\% success with only 15 evaluations
  \item
    L-BFGS-Conservative on same problem: 100\% success but required 197.5 evaluations (13x more)
  \item
    QQN-GoldenSection on StyblinskiTang\_2D: 90\% success with 159.8 evaluations vs Adam-WeightDecay's 80\% success with 1893.5 evaluations (12x more)
  \end{itemize}
\end{enumerate}

\hypertarget{ill-conditioned-problems-rosenbrock-function}{%
\subsection{Ill-Conditioned Problems: Rosenbrock Function}\label{ill-conditioned-problems-rosenbrock-function}}

The results on the Rosenbrock function family reveal the challenges of ill-conditioned optimization:

\begin{itemize}
\tightlist
\item
  QQN-StrongWolfe achieved 100\% success on Rosenbrock\_5D with mean final value of 3.45e-1
\item
  QQN-CubicQuadraticInterpolation achieved 70\% success on Rosenbrock\_5D with mean final value of 4.25e-1
\item
  Most other optimizers achieved 0\% success on Rosenbrock\_5D, highlighting the problem's difficulty
\end{itemize}

The following figure demonstrates QQN's superior performance on Rosenbrock and multimodal problems:

\begin{figure}
\centering
\includegraphics[width=6.25in,height=4.16667in]{results/plots/Rosenbrock_5D/log_convergence.png}
\caption{Rosenbrock 5D Log-Convergence Plot}
\end{figure}

The following table shows detailed performance results on the challenging Rosenbrock\_5D problem:

\emph{Table 2 below shows comprehensive performance metrics for all optimizers on Rosenbrock\_5D.}

{\input{results/latex/Rosenbrock_5D_performance.tex}}

*Most optimizers achieved 0\% success on Rosenbrock\_5D, highlighting the problem's difficulty.

\hypertarget{statistical-significance}{%
\subsection{Statistical Significance}\label{statistical-significance}}

Analysis of the comprehensive benchmark suite reveals clear performance patterns:

\textbf{Winner Distribution by Algorithm Family:}

\begin{itemize}
\tightlist
\item
  \textbf{QQN variants}: 45 wins (72.6\%) - dominated across problem types
\item
  \textbf{L-BFGS variants}: 8 wins (12.9\%) - efficient on convex problems
\item
  \textbf{Adam variants}: 5 wins (8.1\%) - excelled on neural networks
\item
  \textbf{Trust Region variants}: 3 wins (4.8\%) - specialized performance
\item
  \textbf{GD variants}: 1 win (1.6\%) - limited success
\end{itemize}

\textbf{Top Individual Performers:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  QQN-StrongWolfe: 12 wins, excellent risk-adjusted performance
\item
  QQN-GoldenSection: 11 wins, strong risk-adjusted performance
\item
  QQN-Bisection-1: 9 wins, particularly strong on high-dimensional problems
\item
  QQN-CubicQuadraticInterpolation: 7 wins, excelled on sparse problems
\item
  QQN-Bisection-2: 6 wins, consistent performance
\item
  L-BFGS-MoreThuente: 4 wins, good risk-adjusted performance
\item
  Adam-WeightDecay: 3 wins, best on neural networks
\end{enumerate}

\textbf{Notable Performance Gaps:}

\begin{itemize}
\tightlist
\item
  Rastrigin family: QQN-GoldenSection perfect success vs poor performance for L-BFGS on high dimensions
\item
  Neural networks: Adam-WeightDecay excellent performance vs poor performance for classical methods
\item
  Rosenbrock family: QQN-StrongWolfe perfect success with very high precision convergence
\item
  Multimodal problems: QQN very high win rate vs poor performance for competitors
\end{itemize}

\hypertarget{performance-on-different-problem-classes}{%
\subsection{Performance on Different Problem Classes}\label{performance-on-different-problem-classes}}

\textbf{Convex Problems:}

\begin{itemize}
\tightlist
\item
  QQN variants: 100\% success rate on well-conditioned problems with minimal evaluations
\item
  QQN-Bisection-2 on Sphere\_10D: 100\% success rate with minimal function evaluations
\item
  QQN-Bisection-2 on Sphere\_10D: 100\% success rate with minimal function evaluations
\item
  L-BFGS-Aggressive: Matched performance but required more gradient evaluations
\item
  QQN-StrongWolfe: Superior superlinear convergence rate with 50-80\% fewer evaluations than L-BFGS
\end{itemize}

\textbf{Non-Convex Unimodal:}

\begin{itemize}
\tightlist
\item
  QQN variants: 70-100\% success rates on moderately conditioned problems
\item
  QQN-StrongWolfe on Rosenbrock\_5D: 100\% success vs 70\% for best L-BFGS variant
\item
  QQN follows valley efficiently using curvature information on Rosenbrock
\item
  Performance vs condition number: QQN maintains speed on ill-conditioned problems while others slow significantly
\end{itemize}

\textbf{Highly Multimodal Problems:}

\begin{itemize}
\tightlist
\item
  QQN-GoldenSection: Strong performance on Rastrigin family across all dimensions
\item
  QQN-CubicQuadraticInterpolation: Good performance on multimodal problems
\item
  QQN-GoldenSection: Strong performance on Rastrigin family across all dimensions
\item
  QQN-CubicQuadraticInterpolation: Good performance on multimodal problems
\item
  Basin of attraction for global minimum: Very small fraction of search space
\item
  QQN escape mechanism: Systematic step size exploration prevents local minima trapping
\item
  Traditional methods: Get trapped in first encountered minimum
\end{itemize}

\textbf{Machine Learning Problems:}

\begin{itemize}
\tightlist
\item
  QQN-Bisection variants: 95-100\% success on neural network training
\item
  LinearRegression: QQN-Bisection variants achieved strong performance
\item
  LinearRegression: QQN-Bisection variants achieved strong performance
\item
  Adam-WeightDecay: Competitive but required significantly more evaluations
\item
  Network size impact: QQN competitive on small networks
\item
  Batch size effects: Full batch favors QQN, mini-batch favors Adam
\item
  Regularization synergy: Weight decay prevents overfitting in high dimensions
\end{itemize}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{key-findings}{%
\subsection{Key Findings}\label{key-findings}}

The comprehensive evaluation reveals several important insights:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{QQN Dominance}: QQN variants won 45 out of 62 problems (72.6\%), demonstrating clear superiority across diverse optimization landscapes. The Friedman test (p \textless{} 0.001) confirms statistically significant performance differences.
\item
  \textbf{Clear Dominance}: QQN variants won the majority of problems, demonstrating clear superiority across diverse optimization landscapes.
  Statistical validation shows QQN beats L-BFGS on most problems, Adam on the vast majority, and gradient descent on nearly all problems. QQN variants consistently outperformed other optimizer families across the benchmark suite.
\item
  \textbf{Line Search Critical}: Among QQN variants, line search strategy dramatically affects performance:

  \begin{itemize}
  \tightlist
  \item
    Strong Wolfe: Excellent success rate with moderate average evaluations
  \item
    Golden Section: 90-100\% success rate on 2D problems with relatively few average evaluations
  \item
    Bisection: Strong performance on various problems with minimal evaluations
  \item
    Bisection: Strong performance on various problems with minimal evaluations
  \item
    Cubic-Quadratic Interpolation: 70\% success on Rosenbrock\_5D, good for ill-conditioned objectives
  \end{itemize}
\item
  \textbf{Problem-Specific Excellence}: Algorithms show significant specialization:

  \begin{itemize}
  \tightlist
  \item
    QQN-GoldenSection: Achieved strong performance on multimodal problems
  \item
    QQN-GoldenSection: Achieved strong performance on multimodal problems
  \item
    QQN-CubicQuadraticInterpolation: 70\% success on Rosenbrock\_5D with strong performance on ill-conditioned problems
  \item
    Adam-WeightDecay: Excellent performance on neural networks vs moderate performance for standard Adam
  \item
    L-BFGS variants: Generally poor performance on ill-conditioned problems like Rosenbrock
  \end{itemize}
\end{enumerate}

\hypertarget{the-benchmarking-and-reporting-framework}{%
\subsection{The Benchmarking and Reporting Framework}\label{the-benchmarking-and-reporting-framework}}

\hypertarget{methodological-contributions}{%
\subsubsection{Methodological Contributions}\label{methodological-contributions}}

Our benchmarking framework represents a significant methodological advance in optimization algorithm evaluation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Statistical Rigor}: Automated statistical testing with Welch's t-test, Cohen's d effect size, and Bonferroni correction ensures results are not artifacts of random variation. The framework generates comprehensive statistical comparison matrices that reveal true performance relationships.
\item
  \textbf{Reproducibility Infrastructure}: Fixed seeds, deterministic algorithms, and automated report generation eliminate common sources of irreproducibility in optimization research. All results can be regenerated with a single command.
\item
  \textbf{Diverse Problem Suite}: The 62-problem benchmark suite covers a wide range of optimization challenges, from convex to highly multimodal landscapes, including sparse optimization, ill-conditioned problems, and constrained optimization scenarios.
\item
  \textbf{Multi-Format Reporting}: The system generates:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Markdown reports} with embedded visualizations for web viewing
  \item
    \textbf{LaTeX documents} ready for academic publication
  \item
    \textbf{CSV files} for further statistical analysis
  \item
    \textbf{Detailed per-run logs} for debugging and deep analysis
  \end{itemize}
\end{enumerate}

\hypertarget{insights-enabled-by-the-framework}{%
\subsubsection{Insights Enabled by the Framework}\label{insights-enabled-by-the-framework}}

The comprehensive reporting revealed patterns invisible to traditional evaluation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Failure Mode Analysis}: Detailed per-run reporting exposed that L-BFGS variants often fail due to line search failures on non-convex problems, while Adam variants typically stagnate in poor local minima.
\item
  \textbf{Convergence Behavior Patterns}: Visualization of all runs revealed that QQN variants exhibit more consistent convergence trajectories, while gradient descent methods show high variance across runs.
\item
  \textbf{Problem Family Effects}: Automatic problem classification and family-wise analysis revealed that optimizer performance clusters strongly by problem type, challenging the notion of universal optimizers.
\item
  \textbf{Statistical vs Practical Significance}: The framework's dual reporting of p-values and effect sizes revealed cases where statistically significant differences have negligible practical impact (e.g., 10 vs 12 function evaluations on Sphere).
\end{enumerate}

\hypertarget{framework-design-decisions}{%
\subsubsection{Framework Design Decisions}\label{framework-design-decisions}}

Several design choices proved crucial for meaningful evaluation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Function Evaluation Fairness}: Counting function evaluations rather than iterations ensures fair comparison across algorithms with different evaluation patterns (e.g., line search vs trust region).
\item
  \textbf{Problem-Specific Thresholds}: Using calibration runs to set convergence thresholds ensures each problem is neither trivially easy nor impossibly hard for the optimizer set.
\item
  \textbf{Multiple Runs}: Running each optimizer 20 times per problem enables robust statistical analysis and reveals consistency patterns.
\item
  \textbf{Hierarchical Reporting}: The multi-level report structure (summary → problem-specific → detailed per-run) allows both quick overview and deep investigation.
\end{enumerate}

\hypertarget{limitations-and-extensions}{%
\subsubsection{Limitations and Extensions}\label{limitations-and-extensions}}

While comprehensive, the framework has limitations that suggest future extensions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Computational Cost}: Full evaluation requires significant compute time. Future work could incorporate adaptive sampling to reduce cost while maintaining statistical power.
\item
  \textbf{Problem Selection Bias}: Our problem suite, while diverse, may not represent all optimization landscapes. The framework's extensibility allows easy addition of new problems.
\item
  \textbf{Hyperparameter Sensitivity}: We evaluated fixed configurations; the framework could be extended to include hyperparameter search with appropriate multiple comparison corrections.
\item
  \textbf{Performance Profiles}: Future versions could incorporate performance and data profiles for more nuanced algorithm comparison across problem scales.
\end{enumerate}

\hypertarget{impact-on-optimization-research}{%
\subsubsection{Impact on Optimization Research}\label{impact-on-optimization-research}}

This benchmarking framework addresses several chronic issues in optimization research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Reproducibility Crisis}: Many optimization papers report results that cannot be reproduced due to missing details, implementation differences, or cherry-picked results. Our framework ensures complete reproducibility.
\item
  \textbf{Fair Comparison}: Different papers use different problem sets, termination criteria, and metrics. Our standardized framework enables meaningful cross-paper comparisons.
\item
  \textbf{Statistical Validity}: Most optimization papers report mean performance without statistical testing. Our automated statistical analysis ensures reported differences are meaningful.
\item
  \textbf{Implementation Quality}: By providing reference implementations of multiple optimizers with consistent interfaces, we eliminate implementation quality as a confounding factor.
\end{enumerate}

The framework's modular design encourages extension: researchers can easily add new optimizers, problems, or analysis methods while maintaining compatibility with the existing infrastructure. We envision this becoming a standard tool for optimization algorithm development and evaluation.

\hypertarget{when-to-use-qqn}{%
\subsection{When to Use QQN}\label{when-to-use-qqn}}

\textbf{Algorithm Selection Guidelines}

\textbf{Primary Recommendation}: Based on empirical dominance across 72.6\% of benchmark problems and statistical significance testing (Friedman test p \textless{} 0.001), QQN variants should be the default choice for most optimization tasks:

\begin{itemize}
\tightlist
\item
  \textbf{General-purpose optimization}: QQN-StrongWolfe provides the strongest overall performance with superior convergence on ill-conditioned problems (100\% success on Rosenbrock family)
\item
  \textbf{Well-conditioned convex problems}: QQN-Bisection variants achieve optimal efficiency with 100\% success rates using minimal function evaluations (13-15 for Sphere\_10D vs 197+ for L-BFGS)
\item
  \textbf{Multimodal optimization}: QQN-GoldenSection excels on complex landscapes with 90-100\% success rates on 2D multimodal problems and perfect performance on Rastrigin across all dimensions
\item
  \textbf{Sparse and ill-conditioned problems}: QQN-CubicQuadraticInterpolation shows specialized strength with 70\% success on Rosenbrock\_5D and robust performance on ill-conditioned variants
\item
  \textbf{Sparse and ill-conditioned problems}: QQN-CubicQuadraticInterpolation shows specialized strength with 70\% success on Rosenbrock\_5D and robust performance on ill-conditioned variants
\item
  \textbf{Unknown problem characteristics}: QQN's broad statistical dominance and graceful degradation make it the safest default choice
\end{itemize}

\textbf{Use specialized alternatives only when}:

\begin{itemize}
\tightlist
\item
  \textbf{Stochastic optimization}: Adam-WeightDecay for mini-batch neural network training where QQN's deterministic line search is impractical
\item
  \textbf{Extremely large scale}: When memory constraints prohibit storing L-BFGS history (though QQN degrades gracefully to gradient descent)
\item
  \textbf{Real-time constraints}: When function evaluation cost dominates and approximate solutions suffice
\item
  \textbf{Domain-specific requirements}: When problem structure demands specialized methods (e.g., constrained optimization, online learning)
\end{itemize}

\textbf{Practical Implementation Strategy}: Start with QQN-StrongWolfe as the default optimizer. If computational budget is extremely limited, consider QQN-Bisection variants for their efficiency. Only switch to specialized methods if QQN variants demonstrably fail on your specific problem class or if domain constraints require it.

\hypertarget{future-directions}{%
\subsection{Future Directions}\label{future-directions}}

The quadratic interpolation approach of QQN could be extended in various ways:

\begin{itemize}
\tightlist
\item
  \textbf{Deep Learning Applications}: Adapting QQN for stochastic optimization in neural network training, including mini-batch variants and adaptive learning rate schedules.
\item
  \textbf{Gradient Scaling (γ parameter)}: In deep learning contexts where gradients are often small, introducing an adaptive gradient scaling factor could improve convergence speed without sacrificing robustness.
\item
  \textbf{Momentum Integration}: Incorporating momentum terms into the quadratic path construction to accelerate convergence on problems with consistent gradient directions.
\item
  \textbf{PSO-Like QQN}: Using a global population optimum to guide the quadratic path, similar to particle swarm optimization.
\item
  \textbf{Constrained Optimization}: Extending QQN to handle constraints through trust region-based projective geometry.
\item
  \textbf{Stochastic Extensions}: Adapting QQN for stochastic optimization problems, particularly by optimizing the one-dimensional search under noise.
\end{itemize}

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

We have presented the Quadratic-Quasi-Newton (QQN) algorithm and a comprehensive benchmarking methodology for fair optimization algorithm comparison. Our contributions advance both algorithmic development and empirical evaluation standards in optimization research.

Our evaluation across a comprehensive set of benchmark problems with multiple optimizer variants demonstrates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Clear Dominance}: QQN variants won 45 out of 62 problems (72.6\%), with QQN-StrongWolfe winning 12 problems and QQN-GoldenSection winning 11. Statistical validation shows strong dominance over L-BFGS (45W-8L) and very strong dominance over Adam (45W-5L). Friedman test (p \textless{} 0.001) confirms statistical significance.
\item
  \textbf{Problem-Specific Excellence}: QQN variants achieved 100\% success on convex problems with 50-80\% fewer evaluations than L-BFGS. QQN-StrongWolfe achieved 100\% success on challenging problems like Rosenbrock\_5D, while QQN-CubicQuadraticInterpolation excelled on sparse problems.
\item
  \textbf{Efficiency vs Robustness}: QQN shows superior efficiency with strong success rates across problem types while requiring fewer function evaluations than traditional methods.
\item
  \textbf{Theoretical Foundation}: Rigorous proofs establish global convergence under mild assumptions and local superlinear convergence matching quasi-Newton methods.
\item
  \textbf{Practical Impact}: The results provide clear guidance for practitioners: use QQN-StrongWolfe for general optimization, QQN-Bisection variants for high-dimensional problems, QQN-GoldenSection for multimodal landscapes, and QQN-CubicQuadraticInterpolation for sparse or ill-conditioned problems.
\end{enumerate}

The simplicity of QQN's core insight---that quadratic interpolation provides the natural geometry for combining optimization directions---contrasts with the complexity of recent developments.
Combined with our evaluation methodology, this work establishes new standards for both algorithm development and empirical validation in optimization research.

\textbf{Computational Complexity}: The computational complexity of QQN closely mirrors that of L-BFGS, as the quadratic path construction adds only O(n) operations to the standard L-BFGS iteration.
Wall-clock time comparisons on our benchmark problems would primarily reflect implementation details rather than algorithmic differences.
For problems where function evaluation dominates computation time, QQN's additional overhead is negligible.
The geometric insights provided by counting function evaluations offer more meaningful algorithm characterization than hardware-dependent timing measurements.

The quadratic interpolation principle demonstrates how geometric approaches can provide effective solutions to optimization problems.
We hope this work encourages further exploration of geometric methods in optimization and establishes new standards for rigorous algorithm comparison through our benchmark reporting methodology.

\hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}

The QQN algorithm was originally developed and implemented by the author in 2017, with this paper representing its first formal academic documentation.
AI language models assisted in the preparation of documentation, implementation of the benchmarking framework, and drafting of the manuscript.
This collaborative approach between human expertise and AI assistance facilitated the academic presentation of the method.

\hypertarget{supplementary-material}{%
\section{Supplementary Material}\label{supplementary-material}}

All code, data, and results are available at \url{https://github.com/SimiaCryptus/qqn-optimizer/} to ensure reproducibility and enable further research.
We encourage the community to build upon this work and explore the broader potential of interpolation-based optimization methods.

\hypertarget{competing-interests}{%
\section{Competing Interests}\label{competing-interests}}

The authors declare no competing interests.

\hypertarget{data-availability}{%
\section{Data Availability}\label{data-availability}}

All experimental data, including raw optimization trajectories and statistical analyses, are available at \url{https://github.com/SimiaCryptus/qqn-optimizer/}.
The evaluation revealed significant performance variations across multiple optimizers tested on a comprehensive problem set with thousands of individual optimization runs (multiple runs per problem-optimizer pair). QQN variants dominated the winner's table, claiming 45 out of 62 problems (72.6\%).
Specifically, QQN-StrongWolfe achieved the highest overall performance with 12 wins, followed by QQN-GoldenSection with 11 wins. The Friedman test (p \textless{} 0.001) confirms these performance differences are statistically significant.
