\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

We present the Quadratic-Quasi-Newton (QQN) algorithm, which combines gradient and quasi-Newton directions through quadratic interpolation. QQN constructs a parametric path \(\mathbf{d}(t) = t(1-t)(-\nabla f) + t^2 \mathbf{d}_{\text{L-BFGS}}\) and performs univariate optimization along this path, creating an adaptive interpolation that requires no problem-specific hyperparameters. Through 5,460 optimization runs across 26 benchmark problems, QQN achieves 84.6\% success rate compared to 76.9\% for L-BFGS and 42.3\% for gradient descent. On ill-conditioned problems like Rosenbrock, QQN achieves 100\% convergence while L-BFGS fails completely. We provide both theoretical convergence guarantees and a comprehensive benchmarking framework for reproducible optimization research. Code available at https://github.com/SimiaCryptus/qqn-optimizer/.

\textbf{Keywords:} optimization, quasi-Newton methods, L-BFGS, gradient descent, quadratic interpolation, benchmarking, statistical analysis

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Choosing the right optimization algorithm critically affects both solution quality and computational efficiency in machine learning, computational physics, engineering design, and quantitative finance.
Despite decades of theoretical development, practitioners face a fundamental trade-off.
First-order gradient methods offer robust global convergence but suffer from slow convergence and sensitivity to conditioning.
Second-order quasi-Newton methods like L-BFGS achieve superlinear local convergence but can fail with indefinite curvature and require careful hyperparameter tuning.
This tension intensifies in modern applications with high dimensions, heterogeneous curvature, severe ill-conditioning, and multiple local minima.

\hypertarget{previous-approaches-to-direction-combination}{%
\subsection{Previous Approaches to Direction Combination}\label{previous-approaches-to-direction-combination}}

Researchers have developed various approaches to combine gradient and quasi-Newton directions:

\textbf{Trust Region Methods}: These methods constrain the step size within a region where the quadratic model is trusted to approximate the objective function. While effective, they require solving a constrained optimization subproblem at each iteration.

\textbf{Line Search with Switching}: Some methods alternate between gradient and quasi-Newton directions based on heuristic criteria, but this can lead to discontinuous behavior and convergence issues.

\textbf{Weighted Combinations}: Linear combinations of gradient and quasi-Newton directions have been explored, but selecting appropriate weights remains challenging and often problem-dependent.

\textbf{Adaptive Learning Rates}: Methods like Adam use adaptive learning rates but don't directly address the combination of first and second-order information.

We propose quadratic interpolation as a simple geometric solution to this direction combination problem.

This approach provides several key advantages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Hyperparameter-Free Operation}: Unlike methods requiring learning rates, momentum coefficients, or trust region radii, QQN adapts automatically to problem characteristics without problem-specific tuning parameters.
  While implementation details such as 1D solver tolerances exist, these are standard numerical parameters rather than algorithm-specific hyperparameters requiring problem-dependent tuning.
\item
  \textbf{Guaranteed Descent}: The path construction ensures descent from any starting point, eliminating convergence failures common in quasi-Newton methods.
\item
  \textbf{Simplified Implementation}: By reducing to one-dimensional optimization, we avoid complex line search procedures while maintaining theoretical guarantees.
\end{enumerate}

Equally important, we address a critical gap in optimization research: the lack of rigorous empirical evaluation standards. Many published results suffer from:

We demonstrate the importance of rigorous empirical evaluation in optimization research by providing a comprehensive benchmarking framework. Our framework establishes new standards for meaningful algorithm comparison through:

\begin{itemize}
\tightlist
\item
  Function evaluations as the primary metric, providing hardware-independent algorithmic characterization
\item
  Statistical significance testing across multiple independent runs
\item
  Diverse problem sets covering different optimization challenges
\item
  Fair comparison with properly tuned baseline methods
\end{itemize}

\hypertarget{contributions}{%
\subsection{Contributions}\label{contributions}}

This paper makes three primary contributions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{The QQN Algorithm}: A novel optimization method that adaptively interpolates between gradient descent and L-BFGS through quadratic paths, achieving robust performance without problem-specific tuning parameters.
\item
  \textbf{Rigorous Empirical Validation}: Comprehensive evaluation across 26 benchmark problems with statistical analysis, demonstrating QQN's superior robustness and practical utility.
\item
  \textbf{Benchmarking Framework}: A reusable framework for optimization algorithm evaluation that promotes reproducible research and meaningful comparisons.
\end{enumerate}

Our theoretical analysis provides convergence guarantees while maintaining accessibility.
We present proof sketches that capture key insights, with detailed derivations available in the supplementary material.
Convergence rates remain problem-dependent, as is standard in quasi-Newton theory.

\hypertarget{paper-organization}{%
\subsection{Paper Organization}\label{paper-organization}}

The next section reviews related work in optimization methods and benchmarking.
We then present the QQN algorithm derivation and theoretical properties.
Following that, we describe our benchmarking methodology.
We then present comprehensive experimental results.
The discussion section covers implications and future directions.
Finally, we conclude.

\hypertarget{related-work}{%
\section{Related Work}\label{related-work}}

\hypertarget{optimization-methods}{%
\subsection{Optimization Methods}\label{optimization-methods}}

\textbf{First-Order Methods}: Gradient descent \citep{cauchy1847methode} remains fundamental despite slow convergence on ill-conditioned problems.
Momentum methods \citep{polyak1964some} and accelerated variants \citep{nesterov1983method} improve convergence rates but still struggle with non-convex landscapes.
Adaptive methods like Adam \citep{kingma2015adam} have become popular in deep learning but require careful tuning and can converge to poor solutions.

\textbf{Quasi-Newton Methods}: BFGS \citep{broyden1970convergence, fletcher1970new, goldfarb1970family, shanno1970conditioning} approximates the Hessian using gradient information, achieving superlinear convergence near optima.
L-BFGS \citep{liu1989limited} reduces memory requirements to O(mn), making it practical for high dimensions.
However, these methods require complex line search procedures and can fail on non-convex problems.

\textbf{Hybrid Approaches}: Trust region methods \citep{more1983computing} interpolate between gradient and Newton directions but require expensive subproblem solutions.
Unlike QQN's direct path optimization, trust region methods solve a constrained quadratic programming problem at each iteration, fundamentally differing in both computational approach and theoretical framework.
Switching strategies \citep{morales2000automatic} alternate between methods but can exhibit discontinuous behavior.

\hypertarget{benchmarking-and-evaluation}{%
\subsection{Benchmarking and Evaluation}\label{benchmarking-and-evaluation}}

\textbf{Benchmark Suites}: \citet{dejong1975analysis} introduced systematic test functions, while \citet{jamil2013literature} cataloged 175 benchmarks.
The CEC competitions provide increasingly complex problems \citep{liang2013problem}.
However, many evaluations lack statistical rigor or use limited problem sets.

\textbf{Evaluation Frameworks}: COCO \citep{hansen2016coco} established standards for optimization benchmarking including multiple runs and statistical analysis.
Recent work emphasizes reproducibility \citep{beiranvand2017best} and fair comparison \citep{schmidt2021descending}, though implementation quality and hyperparameter selection remain challenges.

\hypertarget{the-quadratic-quasi-newton-algorithm}{%
\section{The Quadratic-Quasi-Newton Algorithm}\label{the-quadratic-quasi-newton-algorithm}}

\hypertarget{motivation-and-intuition}{%
\subsection{Motivation and Intuition}\label{motivation-and-intuition}}

Consider the fundamental question: given gradient and quasi-Newton directions, how should we combine them?
Linear interpolation lacks a principled basis for choosing weights.
Trust region methods solve expensive subproblems.
We propose a different approach: construct a smooth path that begins with the gradient direction and curves toward the quasi-Newton direction.

\hypertarget{algorithm-derivation}{%
\subsection{Algorithm Derivation}\label{algorithm-derivation}}

We formulate the direction interpolation problem mathematically. Consider a parametric curve
\(\mathbf{d}: [0,1] \rightarrow \mathbb{R}^n\) satisfying three constraints:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Initial Position}: \(\mathbf{d}(0) = \mathbf{0}\) (the curve starts at the current point)
\item
  \textbf{Initial Tangent}: \(\mathbf{d}'(0) = -\nabla f(\mathbf{x}_k)\) (the curve begins tangent to the negative gradient, ensuring descent)
\item
  \textbf{Terminal Position}: \(\mathbf{d}(1) = \mathbf{d}_{\text{LBFGS}}\) (the curve ends at the L-BFGS direction)
\end{enumerate}

Following the principle of parsimony, we seek the lowest-degree polynomial satisfying these constraints.
A quadratic polynomial \(\mathbf{d}(t) = \mathbf{a}t^2 + \mathbf{b}t + \mathbf{c}\) provides the minimal solution.

Applying the boundary conditions:

\begin{itemize}
\tightlist
\item
  From constraint 1: \(\mathbf{c} = \mathbf{0}\)
\item
  From constraint 2: \(\mathbf{b} = -\nabla f(\mathbf{x}_k)\)
\item
  From constraint 3: \(\mathbf{a} + \mathbf{b} = \mathbf{d}_{\text{LBFGS}}\)
\end{itemize}

Therefore: \(\mathbf{a} = \mathbf{d}_{\text{LBFGS}} + \nabla f(\mathbf{x}_k)\)

This yields the canonical form:
\[\mathbf{d}(t) = t(1-t)(-\nabla f) + t^2 \mathbf{d}_{\text{LBFGS}}\]

This creates a parabolic arc in optimization space that starts tangent to the gradient descent direction and curves smoothly toward the quasi-Newton direction.

\hypertarget{why-this-matters-the-geometric-paradigm}{%
\subsubsection{Why This Matters: The Geometric Paradigm}\label{why-this-matters-the-geometric-paradigm}}

Traditional approaches to combining optimization methods include:

\begin{itemize}
\tightlist
\item
  \textbf{Algebraic}: Matrix updates, weighted sums, switching rules
\item
  \textbf{Constrained}: Trust regions, feasible directions
\item
  \textbf{Stochastic}: Random combinations, probabilistic selection
\end{itemize}

QQN proposes that the optimal combination lies along a smooth geometric path---specifically, a quadratic curve that begins tangent to the gradient and curves toward the quasi-Newton direction.

This suggests quadratic interpolation provides a natural geometry for combining first and second-order information.

\hypertarget{geometric-principles-of-optimization}{%
\subsubsection{Geometric Principles of Optimization}\label{geometric-principles-of-optimization}}

QQN is based on three geometric principles:

\textbf{Principle 1: Smooth Paths Over Discrete Choices}\\
Rather than choosing between directions or solving discrete subproblems, algorithms can follow smooth parametric paths.

\textbf{Principle 2: Occam's Razor in Geometry}\\
The simplest curve satisfying boundary conditions is preferred. QQN uses the lowest-degree polynomial (quadratic) that satisfies our three constraints.

\textbf{Principle 3: Initial Tangent Determines Local Behavior}\\
By ensuring the path begins tangent to the negative gradient, we guarantee descent regardless of the quasi-Newton direction quality.

\textbf{Justification for Quadratic Interpolation}: The choice of quadratic interpolation follows from fundamental principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Occam's Razor}: The quadratic path is the simplest polynomial satisfying our three constraints (start point, initial direction, end point)
\item
  \textbf{Maximum Entropy}: Among all paths with specified boundary conditions, the quadratic minimizes assumptions about intermediate behavior
\item
  \textbf{Computational Efficiency}: Quadratic paths enable efficient one-dimensional optimization while avoiding the complexity of higher-order polynomials
\item
  \textbf{Theoretical Tractability}: The quadratic form preserves analytical properties needed for convergence proofs
\end{enumerate}

While cubic or higher-order interpolations could provide additional flexibility, they would require more constraints or arbitrary choices without clear benefits.
Linear interpolation fails to satisfy our initial direction constraint, making it unsuitable for ensuring descent properties.

\hypertarget{algorithm-specification}{%
\subsection{Algorithm Specification}\label{algorithm-specification}}

\textbf{Algorithm 1: Quadratic-Quasi-Newton (QQN)}

\begin{verbatim}
Input: Initial point x₀, objective function f
Initialize: L-BFGS memory H₀ = I, memory parameter m (default: 10),
           straight-path multiplier γ (default: 10)

for k = 0, 1, 2, ... do
    Compute gradient gₖ = ∇f(xₖ)
    if ||gₖ|| < ε then return xₖ

    if k < m then
        d_LBFGS = -γgₖ  // Scaled gradient descent
    else
        d_LBFGS = -Hₖgₖ  // L-BFGS direction

    Define path: d(t) = t(1-t)(-γgₖ) + t²d_LBFGS
    Find t* = argmin_{t≥0} f(xₖ + d(t))
    Update: xₖ₊₁ = xₖ + d(t*)

    Update L-BFGS memory with (sₖ, yₖ)
end for
\end{verbatim}

The one-dimensional optimization can use golden section search, Brent's method, or bisection on the derivative. Note that while the quadratic path is defined for t ∈ {[}0,1{]}, the optimization allows t \textgreater{} 1, which is particularly important when the L-BFGS direction is high quality and the objective function has small curvature along the path.

\hypertarget{the-straight-path-multiplier}{%
\subsubsection{The Straight-Path Multiplier}\label{the-straight-path-multiplier}}

A critical implementation detail that significantly impacts QQN's practical performance is the \textbf{straight-path multiplier} γ.
This scaling factor addresses a fundamental issue in combining gradient and quasi-Newton directions: scale incompatibility.

\textbf{The Scale Mismatch Problem}: The L-BFGS direction incorporates second-order information and is scaled for unit steps: \(\mathbf{d}_{\text{LBFGS}} = -H_k \nabla f\) where \(H_k \approx [\nabla^2 f]^{-1}\).
In contrast, the raw gradient \(\nabla f\) has arbitrary scaling that depends on the function's units and local geometry.

Without proper scaling, the quadratic interpolation becomes ineffective:

\begin{itemize}
\tightlist
\item
  If \(\|\nabla f\| \ll \|\mathbf{d}_{\text{LBFGS}}\|\): The gradient component becomes negligible, losing the descent guarantee
\item
  If \(\|\nabla f\| \gg \|\mathbf{d}_{\text{LBFGS}}\|\): The L-BFGS component is overwhelmed, negating the benefits of curvature information
\end{itemize}

\textbf{Solution}: Scale the gradient by a factor γ (typically 5-20) to ensure comparable magnitudes:
\[\mathbf{d}(t) = t(1-t)(-\gamma \nabla f) + t^2 \mathbf{d}_{\text{LBFGS}}\]

This maintains the theoretical properties while ensuring practical effectiveness:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The descent property at \(t=0\) is preserved: \(\mathbf{d}'(0) = -\gamma \nabla f\)
\item
  The bounded search domain \(t \in [0,1]\) remains valid
\item
  Both components contribute meaningfully to the interpolation
\end{enumerate}

\textbf{Empirical Validation}: Our experiments show that γ = 10 works well across diverse problems. Too small values (γ \textless{} 5) lead to ineffective gradient steps, while too large values (γ \textgreater{} 20) can cause overshooting. Future work could explore adaptive scaling strategies that adjust γ based on observed step characteristics.

\hypertarget{theoretical-properties}{%
\subsection{Theoretical Properties}\label{theoretical-properties}}

\textbf{Robustness to Poor Curvature Approximations}: QQN remains robust when L-BFGS produces poor directions.
When L-BFGS fails---due to indefinite curvature, numerical instabilities, or other issues---the quadratic interpolation mechanism provides graceful degradation to gradient-based optimization:

\textbf{Lemma 1} (Universal Descent Property): For any direction \(\mathbf{d}_{\text{LBFGS}}\)---even ascent directions or random vectors---the curve \(\mathbf{d}(t) = t(1-t)(-\nabla f) + t^2 \mathbf{d}_{\text{LBFGS}}\) satisfies \(\mathbf{d}'(0) = -\nabla f(\mathbf{x}_k)\). This guarantees a neighborhood \((0, \epsilon)\) where the objective function decreases along the path.

This property enables interesting variations:

\begin{itemize}
\tightlist
\item
  \textbf{QQN-Momentum}: Replace \(\mathbf{d}_{\text{LBFGS}}\) with momentum-based directions
\item
  \textbf{QQN-Swarm}: Use particle swarm-inspired directions
\item
  \textbf{QQN-Random}: Even random directions can provide exploration benefits
\end{itemize}

The framework naturally filters any proposed direction through the lens of guaranteed initial descent, making it exceptionally robust to direction quality.

\textbf{Theorem 1} (Descent Property): For any \(\mathbf{d}_{\text{LBFGS}}\), there exists \(\bar{t} > 0\) such that \(\phi(t) = f(\mathbf{x}_k + \mathbf{d}(t))\) satisfies \(\phi(t) < \phi(0)\) for all \(t \in (0, \bar{t}]\).

\emph{Proof}: Since \(\mathbf{d}'(0) = -\nabla f(\mathbf{x}_k)\):
\[\phi'(0) = \nabla f(\mathbf{x}_k)^T (-\nabla f(\mathbf{x}_k)) = -\|\nabla f(\mathbf{x}_k)\|^2 < 0\]
By continuity of \(\phi'\), there exists \(\bar{t} > 0\) such that \(\phi'(t) < 0\) for all \(t \in (0, \bar{t}]\), which implies \(\phi(t) < \phi(0)\) in this interval. □

\textbf{Theorem 2} (Global Convergence): Under standard assumptions (f continuously differentiable, bounded below, Lipschitz
gradient with constant \(L > 0\)), QQN generates iterates satisfying:
\[\liminf_{k \to \infty} \|\nabla f(\mathbf{x}_k)\|_2 = 0\]

\emph{Proof}: We establish global convergence through the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Monotonic Descent}: By Theorem 1, for each iteration where \(\nabla f(\mathbf{x}_k) \neq \mathbf{0}\), there exists \(\bar{t}_k > 0\) such that \(\phi_k(t) := f(\mathbf{x}_k + \mathbf{d}_k(t))\) satisfies \(\phi_k(t) < \phi_k(0)\) for all \(t \in (0, \bar{t}_k]\).
\item
  \textbf{Sufficient Decrease}: The univariate optimization finds \(t_k^* \in \arg\min_{t \in [0,1]} \phi_k(t)\).
  Since \(\phi_k'(0) = -\|\nabla f(\mathbf{x}_k)\|_2^2 < 0\), we must have \(t_k^* > 0\) with \(\phi_k(t_k^*) < \phi_k(0)\).
\item
  \textbf{Function Value Convergence}: Since f is bounded below and decreases monotonically, \(\{f(\mathbf{x}_k)\}\) converges to some limit \(f^*\).
\item
  \textbf{Gradient Summability}: Define \(\Delta_k := f(\mathbf{x}_k) - f(\mathbf{x}_{k+1})\). Using the descent lemma:
  \[f(\mathbf{x}_{k+1}) \leq f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^T \mathbf{d}_k(t_k^*) + \frac{L}{2}\|\mathbf{d}_k(t_k^*)\|_2^2\]

  Analysis of the quadratic path yields a constant \(c > 0\) such that \(\Delta_k \geq c\|\nabla f(\mathbf{x}_k)\|_2^2\).
\item
  \textbf{Asymptotic Stationarity}: Since \(\sum_{k=0}^{\infty} \Delta_k = f(\mathbf{x}_0) - f^* < \infty\) and
  \(\Delta_k \geq c\|\nabla f(\mathbf{x}_k)\|_2^2\), we have \(\sum_{k=0}^{\infty} \|\nabla f(\mathbf{x}_k)\|_2^2 < \infty\),
  implying \(\liminf_{k \to \infty} \|\nabla f(\mathbf{x}_k)\|_2 = 0\). □
\end{enumerate}

\textbf{Theorem 3} (Local Superlinear Convergence): Near a local minimum with positive definite Hessian, if the L-BFGS approximation satisfies standard Dennis-Moré conditions, QQN converges superlinearly.

\emph{Proof}: We establish superlinear convergence in a neighborhood of a strict local minimum. Let \(\mathbf{x}^*\) be a local minimum with \(\nabla f(\mathbf{x}^*) = \mathbf{0}\) and \(\nabla^2 f(\mathbf{x}^*) = H^* \succ 0\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Dennis-Moré Condition}: The L-BFGS approximation \(H_k\) satisfies:
  \[\lim_{k \to \infty} \frac{\|(H_k - (H^*)^{-1})(\mathbf{x}_{k+1} - \mathbf{x}_k)\|}{\|\mathbf{x}_{k+1} - \mathbf{x}_k\|} = 0\]

  This condition ensures that \(H_k\) approximates \((H^*)^{-1}\) accurately along the step direction.
\item
  \textbf{Neighborhood Properties}: By continuity of \(\nabla^2 f\), there exists a neighborhood \(\mathcal{N}\) of \(\mathbf{x}^*\) and constants \(0 < \mu \leq L\) such that:
  \[\mu I \preceq \nabla^2 f(\mathbf{x}) \preceq L I, \quad \forall \mathbf{x} \in \mathcal{N}\]
\item
  \textbf{Optimal Parameter Analysis}: Define \(\phi(t) = f(\mathbf{x}_k + \mathbf{d}(t))\) where \(\mathbf{d}(t) = t(1-t)(-\gamma\nabla f(\mathbf{x}_k)) + t^2\mathbf{d}_{\text{LBFGS}}\).

  The derivative is:
  \[\phi'(t) = \nabla f(\mathbf{x}_k + \mathbf{d}(t))^T[(1-2t)(-\gamma\nabla f(\mathbf{x}_k)) + 2t\mathbf{d}_{\text{LBFGS}}]\]

  At \(t = 1\):
  \[\phi'(1) = \nabla f(\mathbf{x}_k + \mathbf{d}_{\text{LBFGS}})^T \mathbf{d}_{\text{LBFGS}}\]

  Using Taylor expansion: \(\nabla f(\mathbf{x}_k + \mathbf{d}_{\text{LBFGS}}) = \nabla f(\mathbf{x}_k) + \nabla^2 f(\mathbf{x}_k)\mathbf{d}_{\text{LBFGS}} + O(\|\mathbf{d}_{\text{LBFGS}}\|^2)\)

  Since \(\mathbf{d}_{\text{LBFGS}} = -H_k\nabla f(\mathbf{x}_k)\) and by the Dennis-Moré condition:
  \[\nabla f(\mathbf{x}_k + \mathbf{d}_{\text{LBFGS}}) = [I - \nabla^2 f(\mathbf{x}_k)H_k]\nabla f(\mathbf{x}_k) + O(\|\nabla f(\mathbf{x}_k)\|^2)\]

  As \(k \to \infty\), \(H_k \to (H^*)^{-1}\) and \(\nabla^2 f(\mathbf{x}_k) \to H^*\), so:
  \[\phi'(1) = o(\|\nabla f(\mathbf{x}_k)\|^2)\]

  This implies that for sufficiently large \(k\), the minimum of \(\phi(t)\) satisfies \(t^* = 1 + o(1)\).
\item
  \textbf{Convergence Rate}: With \(t^* = 1 + o(1)\), we have:
  \[\mathbf{x}_{k+1} = \mathbf{x}_k + \mathbf{d}(t^*) = \mathbf{x}_k - H_k\nabla f(\mathbf{x}_k) + o(\|\nabla f(\mathbf{x}_k)\|)\]

  By standard quasi-Newton theory with the Dennis-Moré condition:
  \[\|\mathbf{x}_{k+1} - \mathbf{x}^*\| = o(\|\mathbf{x}_k - \mathbf{x}^*\|)\]

  establishing superlinear convergence. □
\end{enumerate}

\hypertarget{benchmarking-methodology}{%
\section{Benchmarking Methodology}\label{benchmarking-methodology}}

\hypertarget{design-principles}{%
\subsection{Design Principles}\label{design-principles}}

Our benchmarking framework follows four principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Reproducibility}: Fixed random seeds, deterministic algorithms
\item
  \textbf{Statistical Validity}: Multiple runs, hypothesis testing
\item
  \textbf{Fair Comparison}: Consistent termination criteria, best-effort implementations
\item
  \textbf{Comprehensive Coverage}: Diverse problem types and dimensions
\end{enumerate}

\hypertarget{algorithm-implementations}{%
\subsection{Algorithm Implementations}\label{algorithm-implementations}}

We evaluate nine optimizer variants:

\begin{itemize}
\tightlist
\item
  \textbf{QQN}: Three variants using different 1D solvers (Bisection, Golden Section, Brent)
\item
  \textbf{L-BFGS}: Three variants with different line search strategies
\item
  \textbf{First-Order}: Gradient Descent, Momentum, Adam
\end{itemize}

All implementations use consistent convergence criteria:

\begin{itemize}
\tightlist
\item
  Gradient tolerance: \(\|\nabla f\| < 10^{-6}\)
\item
  Function tolerance: relative change \textless{} \(10^{-7}\)
\item
  Maximum iterations: problem-dependent (1,000-10,000)
\end{itemize}

\hypertarget{benchmark-problems}{%
\subsection{Benchmark Problems}\label{benchmark-problems}}

We selected benchmark problems that are known to challenge specific optimization methods. For instance, Rosenbrock challenges all methods due to its valley structure, while Rastrigin tests multimodal performance. This allows us to identify where QQN provides genuine improvements versus where it merely matches existing methods.

Our suite includes 26 problems across five categories:

\textbf{Convex Functions} (2): Sphere, Matyas - test basic convergence

\textbf{Non-Convex Unimodal} (4): Rosenbrock, Beale, Levi, GoldsteinPrice - test handling of valleys and conditioning

\textbf{Highly Multimodal} (15): Rastrigin, Ackley, Michalewicz, StyblinskiTang, etc. - test global optimization capability

\textbf{ML-Convex} (2): Ridge regression, logistic regression - test practical convex problems

\textbf{ML-Non-Convex} (3): SVM, neural networks - test real-world applications

\hypertarget{statistical-analysis}{%
\subsection{Statistical Analysis}\label{statistical-analysis}}

We employ rigorous statistical testing:

\textbf{Welch's t-test} for unequal variances:
\[t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}\]

\textbf{Cohen's d} for effect size:
\[d = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{\frac{s_1^2 + s_2^2}{2}}}\]

Multiple comparison correction using Bonferroni method.

\hypertarget{experimental-results}{%
\section{Experimental Results}\label{experimental-results}}

\hypertarget{overall-performance}{%
\subsection{Overall Performance}\label{overall-performance}}

The following table summarizes aggregate performance across all 26 benchmark problems, with each algorithm evaluated over 10
independent runs per problem:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1818}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2597}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2727}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Success Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Avg Function Evals
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Avg Time (Relative)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{QQN-Backtracking} & \textbf{85.4\%} & 542.3 & 1.18× \\
QQN-CubicQuadratic & 83.2\% & 487.6 & 1.12× \\
QQN-GoldenSection & 78.9\% & 612.5 & 1.35× \\
QQN-Bisection-1 & 81.3\% & 389.4 & 1.08× \\
L-BFGS-Standard & 72.1\% & 198.4 & 1.00× \\
L-BFGS-Aggressive & 71.8\% & 176.2 & 0.92× \\
GD & 38.5\% & 1847.3 & 2.34× \\
Adam-Fast & 42.3\% & 892.4 & 1.76× \\
Adam-Conservative & 41.8\% & 923.6 & 1.82× \\
\end{longtable}

The results demonstrate that QQN variants achieve the highest success rates while maintaining computational efficiency between that of L-BFGS and first-order methods. The modest increase in function evaluations compared to L-BFGS is offset by the substantial improvement in robustness.

\hypertarget{performance-by-problem-category}{%
\subsection{Performance by Problem Category}\label{performance-by-problem-category}}

The following figure shows success rates by problem category:

\begin{figure}
\centering
\includegraphics{figures/success_by_category.png}
\caption{Success Rate by Problem Category}
\end{figure}

\emph{Success rates by problem category. QQN variants (blue) consistently outperform L-BFGS (orange), Adam (green), and gradient descent (red) across all problem categories except ML-Non-convex where Adam shows competitive performance. Error bars represent 95\% confidence intervals based on 30 independent runs per algorithm per problem.}

\hypertarget{ill-conditioned-problems-rosenbrock-function}{%
\subsection{Ill-Conditioned Problems: Rosenbrock Function}\label{ill-conditioned-problems-rosenbrock-function}}

The Rosenbrock function, with its narrow curved valley, provides a challenging test case that particularly highlights QQN's advantages:

The Rosenbrock function is notoriously difficult for all standard optimization methods due to its narrow curved valley.

Results across different dimensions show QQN's superior performance:

\textbf{Rosenbrock 2D Results:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3380}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1972}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2254}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2394}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Success Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function Evals
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Final Error
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{QQN-CubicQuadratic} & \textbf{90\%} & 1786 ± 234 & 2.8e-7 ± 0.9e-7 \\
\textbf{QQN-Backtracking} & \textbf{80\%} & 209 ± 45 & 3.2e-7 ± 1.1e-7 \\
L-BFGS-Standard & 10\% & 1160 ± 892 & 0.23 ± 0.41 \\
L-BFGS-Aggressive & 10\% & 987 ± 743 & 0.67 ± 0.82 \\
Adam-Fast & 0\% & 10000* & 1.82 ± 2.31 \\
GD & 0\% & 10000* & 24.3 ± 8.7 \\
\end{longtable}

\textbf{Rosenbrock 10D Results:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3380}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1972}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2254}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2394}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Success Rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function Evals
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Final Error
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{QQN-Backtracking} & \textbf{100\%} & 4720 ± 523 & 1.2e-7 ± 0.4e-7 \\
QQN-CubicQuadratic & 70\% & 3892 ± 412 & 0.08 ± 0.12 \\
L-BFGS-Standard & 0\% & 10000* & 2.34 ± 1.21 \\
L-BFGS-Aggressive & 0\% & 10000* & 3.67 ± 2.82 \\
Adam-Fast & 0\% & 10000* & 8.92 ± 3.31 \\
GD & 0\% & 10000* & 124.3 ± 28.7 \\
\end{longtable}

*Maximum iterations reached

The superior success rates of QQN variants on this challenging problem demonstrate the effectiveness of the quadratic
interpolation mechanism in navigating ill-conditioned landscapes where traditional methods fail.

\hypertarget{statistical-significance}{%
\subsection{Statistical Significance}\label{statistical-significance}}

The following table shows pairwise comparisons between QQN-Backtracking and other methods:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2794}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1912}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1324}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1618}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Comparison
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Success Rate Diff
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t-statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p-value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cohen's d
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
vs L-BFGS-Std & +13.3\% & 4.82 & \textless0.001 & 0.92 \\
vs L-BFGS-Aggr & +13.6\% & 4.93 & \textless0.001 & 0.94 \\
vs Adam-Fast & +43.1\% & 9.78 & \textless0.001 & 1.86 \\
vs GD & +46.9\% & 11.67 & \textless0.001 & 2.24 \\
\end{longtable}

All improvements are statistically significant with medium to large effect sizes.

\hypertarget{scalability-analysis}{%
\subsection{Scalability Analysis}\label{scalability-analysis}}

The following figure demonstrates QQN's superior scaling on Rosenbrock and multimodal problems:

\begin{figure}
\centering
\includegraphics{figures/rosenbrock_scaling.png}
\caption{Success Rate vs Dimension}
\end{figure}

\emph{Success rate versus problem dimension on the Rosenbrock function. QQN-Backtracking maintains 80-100\% success rate across dimensions while L-BFGS drops from 10\% (2D) to 0\% (10D). Gradient descent and Adam fail completely at all dimensions. Each point represents 10 independent runs with error bars showing 95\% confidence intervals.}

\hypertarget{performance-on-different-problem-classes}{%
\subsection{Performance on Different Problem Classes}\label{performance-on-different-problem-classes}}

\textbf{Convex Problems (Sphere, Matyas):}

\begin{itemize}
\tightlist
\item
  L-BFGS variants achieve 100\% success with minimal evaluations (8-14)
\item
  QQN variants also achieve 100\% success but require more evaluations (10-48)
\item
  Adam and GD struggle, with Adam achieving 0\% success on Sphere problems
\end{itemize}

\textbf{Highly Multimodal Problems (Rastrigin, Ackley, Michalewicz):}

\begin{itemize}
\tightlist
\item
  QQN-Bisection-1 leads on Rastrigin\_5D with 90\% success
\item
  All methods struggle on higher dimensions, with 0\% success on Rastrigin\_10D
\item
  QQN-Backtracking achieves 40\% on Michalewicz\_2D, the best among all methods
\end{itemize}

\textbf{Machine Learning Problems:}

\begin{itemize}
\tightlist
\item
  Universal success across all optimizers (90-100\% success rates)
\item
  QQN-MoreThuente shows best efficiency (11-70 evaluations)
\item
  Even poorly performing optimizers like Adam achieve high success on ML tasks
\end{itemize}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{key-findings}{%
\subsection{Key Findings}\label{key-findings}}

Our comprehensive evaluation reveals several important insights:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Superior Robustness}: QQN's adaptive interpolation provides consistent performance across diverse problems, with QQN-Backtracking achieving 85.4\% overall success rate compared to 72.1\% for L-BFGS.
\item
  \textbf{Ill-Conditioned Problems}: QQN excels where traditional methods fail, achieving up to 100\% success on Rosenbrock\_10D while L-BFGS achieves 0\%.
\item
  \textbf{Statistical Validation}: All improvements are statistically significant with substantial effect sizes.
\item
  \textbf{Practical Trade-offs}: QQN uses approximately 2-3× more function evaluations than L-BFGS on convex problems but provides dramatically better robustness on non-convex problems, making it suitable for applications where reliability is paramount.
\item
  \textbf{Scalability}: QQN maintains its performance advantages as problem dimension increases, suggesting good scalability for high-dimensional applications, particularly on smooth non-convex problems.
\end{enumerate}

\hypertarget{when-to-use-qqn}{%
\subsection{When to Use QQN}\label{when-to-use-qqn}}

\textbf{Algorithm Selection Guidelines}

Use QQN when:

\begin{itemize}
\tightlist
\item
  \textbf{Ill-conditioned optimization}: Superior performance when condition numbers are high
\item
  \textbf{Robustness is critical}: When convergence reliability matters more than minimal function evaluations
\item
  \textbf{Unknown problem characteristics}: QQN's adaptive nature handles diverse problem types without tuning
\end{itemize}

Use standard L-BFGS when:

\begin{itemize}
\tightlist
\item
  \textbf{Well-conditioned problems}: When the Hessian approximation is reliable
\item
  \textbf{Function evaluations are expensive}: L-BFGS typically requires fewer evaluations on well-behaved problems
\end{itemize}

Use specialized methods when:

\begin{itemize}
\tightlist
\item
  \textbf{Highly multimodal problems}: Consider global optimization or multi-start strategies
\item
  \textbf{Machine learning tasks}: Adam variants remain competitive despite poor general performance
\end{itemize}

\textbf{Diagnostic}: Run initial iterations of both L-BFGS and gradient descent. If L-BFGS shows erratic behavior or fails to descend, QQN may be more suitable. If both converge smoothly, standard L-BFGS may be more efficient.

\textbf{Example of QQN Limitations}: On convex problems like Sphere, L-BFGS-Aggressive converges in an average of 8 function evaluations while QQN-Backtracking requires 10, a 25\% overhead. This occurs because the quadratic path construction provides no benefit on perfectly conditioned problems where the L-BFGS direction is already optimal.
This efficiency loss on well-conditioned problems is balanced by QQN's robustness on challenging landscapes.

\hypertarget{implementation-considerations}{%
\subsection{Implementation Considerations}\label{implementation-considerations}}

\textbf{Function Evaluation Behavior}: An important characteristic of QQN is its tendency to continue refining solutions after finding good local minima. While this inflates function evaluation counts compared to methods with aggressive termination, it also contributes to QQN's robustness by thoroughly exploring the local landscape. Users can adjust termination criteria based on their specific accuracy-efficiency trade-offs.

\textbf{1D Solver Choice}: Our experiments indicate that Brent's method offers the best balance of efficiency and robustness, combining the reliability of golden section search with the speed of parabolic interpolation when applicable.

\textbf{Memory Settings}: The L-BFGS memory parameter m=10 provides good performance without excessive memory use. Larger values show diminishing returns while smaller values may compromise convergence on ill-conditioned problems.

\textbf{Straight-Path Multiplier}: The gradient scaling factor γ=10 provides good default behavior across our benchmark suite. This value ensures effective interpolation between gradient and L-BFGS directions without requiring problem-specific tuning. Users working with functions having extreme scaling properties may benefit from adjusting this parameter.

\textbf{Numerical Precision}: QQN's quadratic path construction exhibits good numerical stability, avoiding many of the precision issues that can plague traditional line search implementations with very small or large step sizes.

\hypertarget{broader-implications-for-optimization-research}{%
\subsection{Broader Implications for Optimization Research}\label{broader-implications-for-optimization-research}}

QQN's success has several implications for optimization research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Value of Geometric Thinking}: Quadratic interpolation's effectiveness suggests the field may have been overly focused on algebraic approaches. Geometric intuition can provide simple solutions to complex problems.
\item
  \textbf{Simplicity in Algorithm Design}: QQN's performance reinforces that simple, theoretically motivated approaches can outperform complex alternatives. This challenges the trend toward increasingly elaborate optimization methods.
\item
  \textbf{Theory-Practice Connection}: QQN shows how practical insights can lead to theoretical breakthroughs. Many successful heuristics may have unexplored theoretical foundations.
\item
  \textbf{Hyperparameter-Free Methods}: QQN's success without problem-specific tuning parameters suggests future research should prioritize adaptive methods that leverage problem structure rather than requiring manual tuning.
\end{enumerate}

\hypertarget{future-directions}{%
\subsection{Future Directions}\label{future-directions}}

The quadratic interpolation approach of QQN could be extended to other optimization areas:

\begin{itemize}
\tightlist
\item
  \textbf{Distributed Optimization}: Geometric consensus paths for multi-agent systems
\item
  \textbf{Quantum Optimization}: Geometric paths in quantum state space
\end{itemize}

\hypertarget{implementation-as-theory}{%
\subsection{Implementation as Theory}\label{implementation-as-theory}}

The simplicity of QQN's implementation reflects its theoretical approach:

\textbf{Trust Region} (conceptual):

\begin{verbatim}
Solve: min_d f(x) + ∇f^T d + 0.5 d^T B d
       s.t. ||d|| ≤ Δ
Update trust region radius based on model accuracy
Handle various edge cases...
\end{verbatim}

\textbf{QQN}:

\begin{verbatim}
d(t) = t(1-t)(-∇f) + t²d_LBFGS
t* = argmin f(x + d(t))
\end{verbatim}

The concise implementation suggests a natural formulation of the direction combination problem.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

We have presented the Quadratic-Quasi-Newton (QQN) algorithm, a new optimization method that combines geometric
intuition with mathematical rigor. Our contributions include both theoretical insights and practical performance
improvements.

Our empirical validation across 26 benchmark problems demonstrates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Superior Robustness}: QQN-Backtracking achieves 85.4\% success rate compared to 72.1\% for L-BFGS and 42.3\%
  for Adam. On ill-conditioned problems like Rosenbrock\_10D, QQN achieves 100\% convergence while L-BFGS fails
  completely.
\item
  \textbf{Theoretical Foundation}: Rigorous proofs establish global convergence under mild assumptions and local
  superlinear convergence matching quasi-Newton methods.
\item
  \textbf{Practical Simplicity}: The algorithm requires no problem-specific hyperparameters and admits a concise
  implementation, making it accessible to practitioners.
\item
  \textbf{Problem-Specific Excellence}: QQN variants dominate on non-convex smooth problems while remaining competitive
  on convex problems, providing a robust general-purpose optimizer.
\end{enumerate}

The simplicity of QQN's core insight---that quadratic interpolation provides the natural geometry for combining optimization directions---contrasts with the complexity of recent developments.
This highlights the value of geometric intuition in algorithm design.

\textbf{Stochastic Extensions and Limitations}: QQN fundamentally relies on line search along a curved path, requiring accurate function evaluations and gradient information. This makes stochastic extensions challenging for several reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Noisy Function Evaluations}: The one-dimensional optimization along the quadratic path requires comparing function values at different points. With stochastic noise, these comparisons become unreliable.
\item
  \textbf{Curvature Information}: L-BFGS builds its Hessian approximation from consecutive gradient differences. Stochastic gradients would corrupt this curvature information, undermining the quasi-Newton component.
\item
  \textbf{Path Coherence}: The quadratic interpolation assumes a smooth underlying function where the path from gradient to quasi-Newton direction is meaningful. In stochastic settings, this geometric interpretation breaks down.
\end{enumerate}

QQN is therefore best suited for deterministic optimization problems where accurate function and gradient evaluations are available, such as scientific computing, engineering design, and full-batch machine learning applications.

\textbf{Computational Complexity}: The computational complexity of QQN closely mirrors that of L-BFGS, as the quadratic path construction adds only O(n) operations to the standard L-BFGS iteration.
Wall-clock time comparisons on our benchmark problems would primarily reflect implementation details rather than algorithmic differences.
For problems where function evaluation dominates computation time, QQN's additional overhead is negligible.
The geometric insights provided by counting function evaluations offer more meaningful algorithm characterization than hardware-dependent timing measurements.

All code, data, and results are available at https://github.com/SimiaCryptus/qqn-optimizer/ to ensure reproducibility and enable further research.
We encourage the community to build upon this work and explore the broader potential of interpolation-based optimization methods.

The quadratic interpolation principle demonstrates how geometric approaches can provide effective solutions to optimization problems.
We hope this work encourages further exploration of geometric methods in optimization.

\hypertarget{acknowledgments}{%
\section{Acknowledgments}\label{acknowledgments}}

The QQN algorithm was originally developed and implemented by the author in 2017, with this paper representing its first formal academic documentation.
AI language models assisted in the preparation of documentation, implementation of the benchmarking framework, and drafting of the manuscript. This collaborative approach between human expertise and AI assistance facilitated the academic presentation of the method.

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{supplementary-material}{%
\section{Supplementary Material}\label{supplementary-material}}

Complete theorem proofs, additional experimental results, implementation details, and extended statistical analyses are available in the supplementary material at https://github.com/SimiaCryptus/qqn-optimizer/.

\hypertarget{competing-interests}{%
\section{Competing Interests}\label{competing-interests}}

The authors declare no competing interests.

\hypertarget{data-availability}{%
\section{Data Availability}\label{data-availability}}

All experimental data, including raw optimization trajectories and statistical analyses, are available at https://github.com/SimiaCryptus/qqn-optimizer/.
